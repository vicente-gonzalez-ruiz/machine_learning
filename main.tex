\section{Min-max normalization}
\begin{equation}
  Y = \frac{X-\text{min}(X)}{\text{max}(X)-\text{min}(X)}.
\end{equation}

\section{Cross-entropy}
\begin{equation}
  H(p,q) = -\mathbb{E}_p[\log q],
\end{equation}
where $\mathbb{E}_p$ is the expected value operator with respect to
the distribution $p$.

For discrete probability distributions $p$ and $q$ with the same support $\mathcal{X}$,
\begin{equation}
  H(p,q) = -\sum_{x\in\mathcal{X}}p(x)\log q(x).
\end{equation}

\section{Accuracy (Acc)}
The evaluation metric Accuracy (Acc, \emph{exactitud} in Spanish) evaluates the performance of a
classification model. Specifically, it measures the probability the model
makes correct predictions, and it is numerically defined as
\begin{equation}
  \text{Acc} = \frac{CoPr}{ToPr},
\end{equation}
where, the number of correct predictions
\begin{equation}
  \text{CoPr} = \text{TP} + \text{TN}
\end{equation}
and the total number of predictions
\begin{equation}
  \text{ToPr} = \text{TP} + \text{FP} + \text{TN} + \text{FN},
\end{equation}
where TP is the number of true positives, TN is the number of true
negatives, FP is the number of false positives, and FN is the number
of false negatives.

\section{Recall}
(\emph{recuperación} in Spanish)
\begin{equation}
  \text{R} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\end{equation}

\section{Sensitivity (Se), recall or TPR}
Sensitivity, also known as the True Positive Rate (TPR) and as recall,
reveals how well a model caEn otras palabras, de todas las veces que la respuesta era "sí", ¿cuántas veces el modelo lo detectó?En otras palabras, de todas las veces que la respuesta era "sí", ¿cuántas veces el modelo lo detectó?n identify (classify) positive
instances. It is defined as
\begin{equation}
  \text{Se} = \frac{\text{TP}}{\text{TP} + \text{FN}}.
\end{equation}
In other words, out of all the times the answer was "yes," how many times was the model wrong?

\section{Specifity}
Specifity measures how well a model can correctly identify (classify)
negative instances, that is
\begin{equation}
  \text{Sp} = \frac{\text{TN}}{\text{TN} + \text{FP}}.
\end{equation}

\section{Precision}
This classification metric (\emph{precisión} in Spanish) measures the proportion of correctly predicted positive instances out of all instances predicted as positive, i.e., it is computed by
\begin{equation}
  \text{Pr} = \frac{\text{TP}}{\text{TP} + \text{FP}}.
\end{equation}
In other words, of all the times the model said "yes," how many times was it right?

\section{F1-score}
Classification metric that computes the harmonic mean of Pr and Se:
\begin{equation}
  \text{F1-score} = \frac{2\text{Pr}\text{Se}}{\text{Pr}+\text{Se}}.
\end{equation}
High F1-scores are correlated with a good equilibrium between precision and recall.

\section{Confusion matrix}
A table that summarizes the performance of a classification model by showing
\begin{table}{ccc}
  & Predicted Positive & Predicted Negative \\
  Actual Positive & TP & FN \\
  Actual Negative & FP & TN
\end{table}

\section{}
curva precisión-recuperación
\begin{equation}
  \text{AUC-PRC} = \sum_{i=1}^{t-1}(\text{R}_{i+1}-\text{R}_i)\frac{\text{P}_i+\text{P}_{i+1}}{2}
\end{equation}

\section{Epoch}
An epoch refers to one complete pass through the entire training dataset.

\section{Training dataset}
Collection of data used to teach the model. It's composed of input
features and their corresponding target labels.

\section{Validation dataset}
A subset of the training data used to evaluate the model's performance
when training.

\section{Test dataset}
Data used to evaluate the performance of a trained model.

\section{Batch}
During training, the dataset is ofted divided into smaller chunks
called batches.

\section{Training iteration}
One training iteration occurs when the during the training a single
batch of data has been used.

\section{Overfitting}
Overfitting happens when the model becomes too specialized to the
training dataset, performing poorly on unseen data.

\section{Early stopping}
To prevent overfitting, early stopping fialized the training when the
model's performance on the validation dataset starts to degrade.

\section{Batch normalization}
Batch normalization normalizes the activations of intermediate layers
within a mini-batch of data (only) during the training process. This
means it adjusts the mean and variance of the layer's
inputs. Algorithm:
\begin{enumerate}
\item Calculate batch mean $\mu_B$ and variance $\sigma^2_B$ for the input batch $B$.
\item Normalize the neuron activations using
  \begin{equation}
    \hat{x}_i = \frac{x_i-\mu_B}{\sqrt{\sigma^2_B+\epsilon}},
  \end{equation}
  where $x_i$ is the original  activation value, $\epsilon$ is a small
  constant  to  prevent  division  by zero,  and  $\hat{x}_i$  is  the
  normalized activation value.
\item Scale and shift the normalized activations with
  \begin{equation}
    y_i = \gamma\hat{x}_i+\beta,
  \end{equation}
  where $\gamma$ is a (learnable parameter) scaling factor, and
  $\beta$ (another learnable parameter) shifts the normalized
  activation.
\end{enumerate}

Batch normalization makes the learning process more stable and less
dependent on the initial weights (reduces internal covariate shift),
speed up the training process by allowing higher learning rates,
reduces the probability of vanishing and exploding gradients, acts as
a regularizer (reducing the need for other regularization techniques
like dropout), and reduces dependence on initialization (weights)
values.

\section{Cross-validation}

\section{Five-fold cross-validation}

\section{A word about convolution in DL libraries}
The true convolution of a signal $x$ and a kernel $w$ is defined as
\begin{equation}
  (x*w)[i] = \sum_k x[k]w[i-k].
\end{equation}
Notice that the kernel is \emph{flipped} (reversed). CNNs, however,
compute the cross-correlation, defined as:
\begin{equation}
  (x\star y)[i] = \sum_k x[i+k}w[k],
\end{equation}
of equivalently (depending on indexing):
\begin{equation}
  (x\star y)[i] = \sum_k x[k}w[i+k],
\end{equation}
and therefore, no \emph{flipping} of the kernel happens. In practice,
deep learning libraries (like PyTorch and Tensorflow) what really
compute as a \emph{convolution} is actually a
\emph{cross-correlation}.

\section{Conv1d}
For only one channel, and assuming no padding (``valid'' convolution), it is defined as
\begin{equation}
  y[i] = \sum_{k=0}^{K-1} w[k]x[is+k]+b
\end{equation}
where $x\in\mathcal{R}^L$ is the input digital 1D signal of length
$L$, $w\in\mathbcal{R}^K$ is the kernel (or filter) of length $K$,
$b\in\mathcal{R}$ is the bias, $n\in\mathcal{N}$ is the stride (how
much the filter moves at each step). if the signal $x$ is padded
(extended) with zeros at the boundaries, it is said that we are
computing the``same'' convolution.

When $x\in\mathcal{R}^{C_{\text{in}}L}$ has more than one channel and
a different kernel is used for each channel, then
\begin{equation}
  y[i] = \sum_{c=0}^{C_{\text{in}}-1}\sum_{k=0}^{K-1} w[c,k]x[c,is+k]+b
\end{equation}

Finally, each input channel can be convolved with $C_{\text{out}}$ kernels, generating
\begin{equation}
  y[m,i] = \sum_{c=0}^{C_{\text{in}}-1}\sum_{k=0}^{K-1} w[m,c,k]x[c,is+k]+b[m]
\end{equation}

\section{CNN}
Each convolutional layer learns different patterns and features at
various abstraction levels. The initial layers capture low-level features such
as edges and gradients, whereas the deeper layers capture more complex and
abstract features.