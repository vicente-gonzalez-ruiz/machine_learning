\section{Min-max normalization}
\begin{equation}
  Y = \frac{X-\text{min}(X)}{\text{max}(X)-\text{min}(X)}.
\end{equation}

\section{Cross-entropy}
Cross-entropy is a way to measure the difference between two probability distributions — usually between the true labels and a model’s predicted probabilities.

In ML (especially in classification tasks), cross-entropy is commonly used as a loss function to quantify how well the predicted probability distribution matches the actual distribution (the true class labels).

It can be expressed as
\begin{equation}
  H(p,q) = -\mathbb{E}_p[\log q],
\end{equation}
where $\mathbb{E}_p$ is the expected value operator with respect to
the distribution $p$.

For discrete probability distributions $p$ and $q$ with the same support $\mathcal{X}$,
\begin{equation}
  H(p,q) = -\sum_{x\in\mathcal{X}}p(x)\log q(x),
\end{equation}
where $p(x)$ is the true probability of the label/class $x$ and $q(x)$ is the model predict probability for $x$. When there are only two classes/labels (binary classification), this equation boils down to
\begin{equation}
  H(p,q) = -(p\log q + (1-p)\log(1-q)),
\end{equation}
where $p$ is 0 or 1, and $p$ is the probability of the prediction for label 1.

In classification tasks we usually try to minimize the cross-entropy.

\section{Accuracy (Acc)}
The evaluation metric Accuracy (Acc, \emph{exactitud} in Spanish) evaluates the performance of a
classification model. Specifically, it measures the probability the model
makes correct predictions, and it is numerically defined as
\begin{equation}
  \text{Acc} = \frac{CoPr}{ToPr},
\end{equation}
where, the number of correct predictions
\begin{equation}
  \text{CoPr} = \text{TP} + \text{TN}
\end{equation}
and the total number of predictions
\begin{equation}
  \text{ToPr} = \text{TP} + \text{FP} + \text{TN} + \text{FN},
\end{equation}
where TP is the number of true positives, TN is the number of true
negatives, FP is the number of false positives, and FN is the number
of false negatives.

\section{Recall}
(\emph{recuperación} in Spanish)
\begin{equation}
  \text{R} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\end{equation}

\section{Sensitivity (Se), recall or TPR (True Positive Rate)}
Sensitivity, also known as the True Positive Rate (TPR) and as recall,
reveals how well a model caEn otras palabras, de todas las veces que la respuesta era "sí", ¿cuántas veces el modelo lo detectó?En otras palabras, de todas las veces que la respuesta era "sí", ¿cuántas veces el modelo lo detectó?n identify (classify) positive
instances. It is defined as
\begin{equation}
  \text{Se} = \frac{\text{TP}}{\text{TP} + \text{FN}}.
\end{equation}
In other words, out of all the times the answer was "yes," how many times was the model wrong?

\section{Specifity}
Specifity measures how well a model can correctly identify (classify)
negative instances, that is
\begin{equation}
  \text{Sp} = \frac{\text{TN}}{\text{TN} + \text{FP}}.
\end{equation}

\section{Precision}
This classification metric (\emph{precisión} in Spanish) measures the proportion of correctly predicted positive instances out of all instances predicted as positive, i.e., it is computed by
\begin{equation}
  \text{Pr} = \frac{\text{TP}}{\text{TP} + \text{FP}}.
\end{equation}
In other words, of all the times the model said "yes," how many times was it right?

\section{F1-score}
Classification metric that computes the harmonic mean of Pr and Se:
\begin{equation}
  \text{F1-score} = \frac{2\text{Pr}\text{Se}}{\text{Pr}+\text{Se}}.
\end{equation}
High F1-scores are correlated with a good equilibrium between precision and recall.

\section{Confusion matrix}
A table that summarizes the performance of a classification model by showing
\begin{table}{ccc}
  & Predicted Positive & Predicted Negative \\
  Actual Positive & TP & FN \\
  Actual Negative & FP & TN
\end{table}

\section{}
curva precisión-recuperación
\begin{equation}
  \text{AUC-PRC} = \sum_{i=1}^{t-1}(\text{R}_{i+1}-\text{R}_i)\frac{\text{P}_i+\text{P}_{i+1}}{2}
\end{equation}

\section{False Positive Rate (FPR)}
This is the proportion of actual negative instances that are incorrectly identified as positive.
\begin{equation}
  \text{FPR} = \frac{\text{FP}}{\text{FP}+\text{TN}}
\end{equation}

\section{The ROC curve}
A binary classification model often outputs a probability or score for
each instance, indicating its likelihood of belonging to the positive
class. To classify an instance as positive or negative, a threshold is
applied to this probability. For example, if the probability is above
0.5, it might be classified as positive; otherwise, negative. The ROC
curve is a graphical plot that illustrates the diagnostic ability of a
binary classifier as its discrimination threshold is varied. It plots
two metrics against each other: TPR (y-axis) vs FPR (x-axis).

\section{(Area Under the receiver operating Characteristic (AUC)}
It is a performance metric used primarily for evaluating the effectiveness of binary classification models.

\section{Epoch}
An epoch refers to one complete pass through the entire training dataset.

\section{Training dataset}
Collection of data used to teach the model. It's composed of input
features and their corresponding target labels.

\section{Validation dataset}
A subset of the training data used to evaluate the model's performance
when training.

\section{Test dataset}
Data used to evaluate the performance of a trained model.

\section{Batch}
During training, the dataset is ofted divided into smaller chunks
called batches.

\section{Training iteration}
One training iteration occurs when the during the training a single
batch of data has been used.

\section{Overfitting}
Overfitting happens when the model becomes too specialized to the
training dataset, performing poorly on unseen data.

\section{Early stopping}
To prevent overfitting, early stopping fialized the training when the
model's performance on the validation dataset starts to degrade.

\section{Batch normalization}
Batch normalization normalizes the activations of intermediate layers
within a mini-batch of data (only) during the training process. This
means it adjusts the mean and variance of the layer's
inputs. Algorithm:
\begin{enumerate}
\item Calculate batch mean $\mu_B$ and variance $\sigma^2_B$ for the input batch $B$.
\item Normalize the neuron activations using
  \begin{equation}
    \hat{x}_i = \frac{x_i-\mu_B}{\sqrt{\sigma^2_B+\epsilon}},
  \end{equation}
  where $x_i$ is the original  activation value, $\epsilon$ is a small
  constant  to  prevent  division  by zero,  and  $\hat{x}_i$  is  the
  normalized activation value.
\item Scale and shift the normalized activations with
  \begin{equation}
    y_i = \gamma\hat{x}_i+\beta,
  \end{equation}
  where $\gamma$ is a (learnable parameter) scaling factor, and
  $\beta$ (another learnable parameter) shifts the normalized
  activation.
\end{enumerate}

Batch normalization makes the learning process more stable and less
dependent on the initial weights (reduces internal covariate shift),
speed up the training process by allowing higher learning rates,
reduces the probability of vanishing and exploding gradients, acts as
a regularizer (reducing the need for other regularization techniques
like dropout), and reduces dependence on initialization (weights)
values.

\section{Cross-validation}

\section{Five-fold cross-validation}
Five-fold cross-validation is a common technique used to evaluate the performance of a machine learning model on a limited dataset and to get a more robust estimate of its generalization ability (how well it will perform on unseen data). Here's how it works:

\begin{enumerate}
\item Data Splitting: The original dataset is divided into five roughly equal-sized parts or "folds."

\item Training and Evaluation (Five Iterations): The experiment is run five times (the "five-fold"). In each iteration:
  \begin{enumerate}
  \item One fold is held out as the test set (or validation set). This is the data the model will be evaluated on in this particular run.
  \item The remaining four folds are used to train the model. The model learns from this larger portion of the data.
  \item The trained model is then evaluated on the held-out test fold, and the chosen metrics are recorded.
  \end{enumerate}
\item Averaging the Results: After all five iterations are complete, you will have five sets of metric scores (one from each test fold). The final reported metric in the section is the average of these five scores.
\end{enumerate}

\section{A word about convolution in DL libraries}
The true convolution of a signal $x$ and a kernel $w$ is defined as
\begin{equation}
  (x*w)[i] = \sum_k x[k]w[i-k].
\end{equation}
Notice that the kernel is \emph{flipped} (reversed). CNNs, however,
compute the cross-correlation, defined as:
\begin{equation}
  (x\star y)[i] = \sum_k x[i+k}w[k],
\end{equation}
of equivalently (depending on indexing):
\begin{equation}
  (x\star y)[i] = \sum_k x[k}w[i+k],
\end{equation}
and therefore, no \emph{flipping} of the kernel happens. In practice,
deep learning libraries (like PyTorch and Tensorflow) what really
compute as a \emph{convolution} is actually a
\emph{cross-correlation}.

\section{Conv1d}
For only one channel, and assuming no padding (``valid'' convolution), it is defined as
\begin{equation}
  y[i] = \sum_{k=0}^{K-1} w[k]x[is+k]+b
\end{equation}
where $x\in\mathcal{R}^L$ is the input digital 1D signal of length
$L$, $w\in\mathbcal{R}^K$ is the kernel (or filter) of length $K$,
$b\in\mathcal{R}$ is the bias, $n\in\mathcal{N}$ is the stride (how
much the filter moves at each step). if the signal $x$ is padded
(extended) with zeros at the boundaries, it is said that we are
computing the``same'' convolution.

When $x\in\mathcal{R}^{C_{\text{in}}L}$ has more than one channel and
a different kernel is used for each channel, then
\begin{equation}
  y[i] = \sum_{c=0}^{C_{\text{in}}-1}\sum_{k=0}^{K-1} w[c,k]x[c,is+k]+b
\end{equation}

Finally, each input channel can be convolved with $C_{\text{out}}$ kernels, generating
\begin{equation}
  y[m,i] = \sum_{c=0}^{C_{\text{in}}-1}\sum_{k=0}^{K-1} w[m,c,k]x[c,is+k]+b[m]
\end{equation}

\section{CNN}
Each convolutional layer learns different patterns and features at
various abstraction levels. The initial layers capture low-level features such
as edges and gradients, whereas the deeper layers capture more complex and
abstract features.

\section{Residual network}
For example Resnet \cite{}.

\section{Entropy}
Entropy is a measure of the uncertainty, randomness, or disorder
within a set of data or a probability distribution $X$, that can take
$N$ possible values $\{x_1,x_2,\cdots,x_n\}$. In the context of
information theory, it measures the average number of bits needed to
encode an event from a probability distribution. Entropy can be
computed as
\begin{equation}
  H(X) = -\sum_{i=1}^Mp(x_i)\log_b(p(x_i))
\end{equation}
where $p(x_i)$ is the probability of the $i$-th outcome, and $b$
determines the unit of entropy (``bits'' for $b=2$, ``nats'' for
$b=e$, and ``dits'' or ``hartleys'' for $b=10$).

\section{Cross-entropy}
Cross-entropy is a loss function to measure the difference between two
probability distributions for a given random variable or set of
events. In the context of classification models, it quantifies the
dissimilarity between the predicted probabilities and the true
labels. The lower, the better. The logarithmic nature of the
cross-entropy penalizes confident (small cross-entropy) but incorrect
predictions.

\subsection{Binary cross-entropy (only two classes)}
\begin{equation}
  L = -y\log(p)-(1-y)\log(1-p)
\end{equation}
where $y$ is the true label (0 or 1), and $p$ is the predicted probability of the positive class (class 1).

\subsection{Categorical cross-entropy (for multiple classes)}
\begin{equation}
  L = -\sum_{i=1}^M y_i\log(p_i)
\end{equation}
where $M$ is the number of classes, $y_i$ is the binary indicator (0 or 1) of whether the $i$-th class is the correct label for the data point, and $p_i$ is the predicted probability of the data point belonging to the $i$-th class.

\section{One-hot encoding}

\section{Online learning}

\section{Batch Gradient Descent (BGD)}
Uses the entire dataset to compute the gradient and update parameters
once per epoch. Provides a very accurate estimate of the true
gradient, leading to a smooth convergence path. Extremely
computationally expensive and memory-intensive for large datasets.

\section{Stochastic Gradient Descent (SGD)}
Updates parameters after processing each individual data point. Very
fast updates, can help escape local minima due to the inherent "noise"
in the gradient estimates. Highly noisy updates, leading to a jagged
and potentially unstable convergence path. The gradient from a single
example might not be representative of the overall dataset.

\section{Mini-Batch Gradient Descent (MBGD}
Instead of updating a model's parameters (like weights and biases)
after processing the entire training dataset (as in BGD) or after
processing a single data point (as in SGD), mini-batch training
involves:
\begin{enumerate}
\item Dividing the training dataset into smaller, fixed-size subsets called "mini-batches."
\item For each mini-batch, calculating the gradient of the loss function with respect to the model's parameters.
\item Updating the model's parameters using this calculated gradient.
\item Repeating this process for all mini-batches in an epoch (one full pass through the entire dataset).
\end{enumerate}
MBGD is more computational efficient than BGD and have a metter convergence than SGD. Moreover, the inherent stochasticity (noise) introduced by using mini-batches can act as a form of regularization, helping the model to avoid getting stuck in sharp local minimaq and potentially leading to better generalization to unseen data.

\section{RNN}

\subsection{The Vanishing Gradient Problem in RNN}
Standard RNNs are designed to process sequential data (like text,
speech, time series) by passing information from one step in the
sequence to the next. However, during training, when the error signal
(gradient) is propagated back through many time steps, it can become
extremely small (vanish). This makes it difficult for the network to
learn long-term dependencies – meaning, it struggles to remember
information from much earlier in the sequence. For example, in a long
sentence, a traditional RNN might "forget" the subject by the time it
needs to process the verb.


\section{Gated Recurrent Unit (GRU)}
It's a type of recurrent neural network (RNN) that was introduced to
address some of the limitations of traditional RNNs, particularly the
vanishing gradient problem. GRUs, along with their more complex
counterparts, LSTMs (Long Short-Term Memory networks), introduce
"gates" to control the flow of information. These gates act like
intelligent filters that decide what information to keep, what to
forget, and what to pass on to the next step.

A GRU has two main gates:
\begin{enumerate}
\item Update Gate ($z_t$​): This gate determines how much of the past
  information (from the previous hidden state) should be carried
  forward to the current hidden state, and how much of the new
  candidate hidden state (computed from the current input) should be
  incorporated. Think of it as a "memory update" gate. A value close
  to 1 means "keep a lot of the old information" or "add a lot of the
  new information." A value close to 0 means "forget the old
  information" or "don't add much of the new information."
\item Reset Gate ($r_t$​): This gate decides how much of the previous
  hidden state should be "forgotten" or "reset" before computing the
  new candidate hidden state. It essentially allows the GRU to discard
  irrelevant past information when processing new input. If the reset
  gate outputs a value close to 0, it effectively "resets" or ignores
  the previous hidden state when calculating the current potential
  hidden state.
\end{enumerate}

GRUs are simpler than LSTMs because they combine the forget and input
gates into a single "update" gate and don't have a separate cell
state. They rely solely on the hidden state to transfer
information. This means they have fewer parameters.

\section{Knowledge Graph (KG)}
A KG is a structured representation of knowledge that captures the
relationships between different entities (like people, places, events,
concepts, or objects) or nodes. Examples of nodes are: "Albert
Einstein," "E = mc²," "Relativity Theory," "Germany," "Nobel Prize."
The edges (relationships) are the connections between nodes,
describing how entities are related. Edges are typically directed and
labeled. "Albert Einstein" \textbf{was born in} "Germany," "Albert
Einstein" \textbf{developed} "Relativity Theory," "Albert Einstein"
\textbf{received} "Nobel Prize." Finally, we have labels (properties),
that is additional information or attributes associated with nodes or
edges. Example: The "Nobel Prize" node might have a property "Year: 1921".

\section{Graph Neural Network (GNN)}
A GNN is a specialized type of artificial neural network designed to
operate directly on graph-structured data. Unlike traditional neural
networks (like CNNs for images or RNNs for sequences) that work on
data with a fixed, grid-like, or sequential structure, GNNs can handle
data where elements have complex, non-Euclidean relationships.

Traditional neural networks struggle with graph data because:
\begin{enumerate}
\item Irregular Structure: Graphs don't have a fixed size or a
  consistent order of nodes/edges.
\item Varying Connectivity: A node can have one connection or
  hundreds, unlike a pixel that usually has 4 or 8 neighbors.
\item Rich Relational Information: The relationships (edges)
  themselves often carry important information.
\end{enumerate}

\section{Downstream training}
Those performed only to finetune a pretrained model.
